test -s /home/ubuntu/go/src/kuberay/ray-operator/bin/controller-gen || GOBIN=/home/ubuntu/go/src/kuberay/ray-operator/bin go install sigs.k8s.io/controller-tools/cmd/controller-gen@v0.13.0
/home/ubuntu/go/src/kuberay/ray-operator/bin/controller-gen "crd:maxDescLen=0,generateEmbeddedObjectMeta=true,allowDangerousTypes=true" rbac:roleName=kuberay-operator webhook paths="./..." output:crd:artifacts:config=config/crd/bases
/home/ubuntu/go/src/kuberay/ray-operator/bin/controller-gen object:headerFile="hack/boilerplate.go.txt" paths="./..."
go fmt ./...
go vet ./...
test -s /home/ubuntu/go/src/kuberay/ray-operator/bin/setup-envtest || GOBIN=/home/ubuntu/go/src/kuberay/ray-operator/bin go install sigs.k8s.io/controller-runtime/tools/setup-envtest@latest
KUBEBUILDER_ASSETS="/home/ubuntu/go/src/kuberay/ray-operator/bin/k8s/1.24.2-linux-amd64" go test github.com/ray-project/kuberay/ray-operator github.com/ray-project/kuberay/ray-operator/apis/ray/v1 github.com/ray-project/kuberay/ray-operator/apis/ray/v1alpha1 github.com/ray-project/kuberay/ray-operator/controllers/ray github.com/ray-project/kuberay/ray-operator/controllers/ray/batchscheduler github.com/ray-project/kuberay/ray-operator/controllers/ray/batchscheduler/interface github.com/ray-project/kuberay/ray-operator/controllers/ray/batchscheduler/volcano github.com/ray-project/kuberay/ray-operator/controllers/ray/common github.com/ray-project/kuberay/ray-operator/controllers/ray/utils github.com/ray-project/kuberay/ray-operator/pkg/client/clientset/versioned github.com/ray-project/kuberay/ray-operator/pkg/client/clientset/versioned/fake github.com/ray-project/kuberay/ray-operator/pkg/client/clientset/versioned/scheme github.com/ray-project/kuberay/ray-operator/pkg/client/clientset/versioned/typed/ray/v1 github.com/ray-project/kuberay/ray-operator/pkg/client/clientset/versioned/typed/ray/v1/fake github.com/ray-project/kuberay/ray-operator/pkg/client/informers/externalversions github.com/ray-project/kuberay/ray-operator/pkg/client/informers/externalversions/internalinterfaces github.com/ray-project/kuberay/ray-operator/pkg/client/informers/externalversions/ray github.com/ray-project/kuberay/ray-operator/pkg/client/informers/externalversions/ray/v1 github.com/ray-project/kuberay/ray-operator/pkg/client/listers/ray/v1 -coverprofile cover.out -test.v
?   	github.com/ray-project/kuberay/ray-operator	[no test files]
?   	github.com/ray-project/kuberay/ray-operator/controllers/ray/batchscheduler	[no test files]
?   	github.com/ray-project/kuberay/ray-operator/controllers/ray/batchscheduler/interface	[no test files]
?   	github.com/ray-project/kuberay/ray-operator/pkg/client/clientset/versioned	[no test files]
?   	github.com/ray-project/kuberay/ray-operator/pkg/client/clientset/versioned/fake	[no test files]
?   	github.com/ray-project/kuberay/ray-operator/pkg/client/clientset/versioned/scheme	[no test files]
?   	github.com/ray-project/kuberay/ray-operator/pkg/client/clientset/versioned/typed/ray/v1	[no test files]
?   	github.com/ray-project/kuberay/ray-operator/pkg/client/clientset/versioned/typed/ray/v1/fake	[no test files]
?   	github.com/ray-project/kuberay/ray-operator/pkg/client/informers/externalversions	[no test files]
?   	github.com/ray-project/kuberay/ray-operator/pkg/client/informers/externalversions/internalinterfaces	[no test files]
?   	github.com/ray-project/kuberay/ray-operator/pkg/client/informers/externalversions/ray	[no test files]
?   	github.com/ray-project/kuberay/ray-operator/pkg/client/informers/externalversions/ray/v1	[no test files]
?   	github.com/ray-project/kuberay/ray-operator/pkg/client/listers/ray/v1	[no test files]
=== RUN   TestMarshalling
--- PASS: TestMarshalling (0.00s)
=== RUN   TestMarshallingRayJob
--- PASS: TestMarshallingRayJob (0.00s)
=== RUN   TestMarshallingRayService
--- PASS: TestMarshallingRayService (0.00s)
=== RUN   TestAPIs
Running Suite: Webhook Suite - /home/ubuntu/go/src/kuberay/ray-operator/apis/ray/v1
===================================================================================
Random Seed: [1m1702496551[0m

Will run [1m2[0m of [1m2[0m specs
[38;5;10mâ€¢[0m[38;5;10mâ€¢[0m

[38;5;10m[1mRan 2 of 2 Specs in 14.991 seconds[0m
[38;5;10m[1mSUCCESS![0m -- [38;5;10m[1m2 Passed[0m | [38;5;9m[1m0 Failed[0m | [38;5;11m[1m0 Pending[0m | [38;5;14m[1m0 Skipped[0m
--- PASS: TestAPIs (14.99s)
PASS
coverage: 10.9% of statements
ok  	github.com/ray-project/kuberay/ray-operator/apis/ray/v1	15.083s	coverage: 10.9% of statements
=== RUN   TestMarshalling
--- PASS: TestMarshalling (0.00s)
=== RUN   TestMarshallingRayJob
--- PASS: TestMarshallingRayJob (0.00s)
=== RUN   TestMarshallingRayService
--- PASS: TestMarshallingRayService (0.00s)
PASS
coverage: 0.7% of statements
ok  	github.com/ray-project/kuberay/ray-operator/apis/ray/v1alpha1	0.034s	coverage: 0.7% of statements
=== RUN   TestAPIs
Running Suite: Controller Suite - /home/ubuntu/go/src/kuberay/ray-operator/controllers/ray
==========================================================================================
Random Seed: [1m1702496552[0m

Will run [1m19[0m of [1m19[0m specs
2023-12-13T19:42:32Z	DEBUG	controller-runtime.test-env	starting control plane
2023-12-13T19:42:40Z	DEBUG	controller-runtime.test-env	installing CRDs
2023-12-13T19:42:40Z	DEBUG	controller-runtime.test-env	reading CRDs from path	{"path": "../../config/crd/bases"}
2023-12-13T19:42:41Z	DEBUG	controller-runtime.test-env	read CRDs from file	{"file": "ray.io_rayclusters.yaml"}
2023-12-13T19:42:41Z	DEBUG	controller-runtime.test-env	read CRDs from file	{"file": "ray.io_rayjobs.yaml"}
2023-12-13T19:42:41Z	DEBUG	controller-runtime.test-env	read CRDs from file	{"file": "ray.io_rayservices.yaml"}
2023-12-13T19:42:41Z	DEBUG	controller-runtime.test-env	installing CRD	{"crd": "rayjobs.ray.io"}
2023-12-13T19:42:42Z	DEBUG	controller-runtime.test-env	installing CRD	{"crd": "rayservices.ray.io"}
2023-12-13T19:42:43Z	DEBUG	controller-runtime.test-env	installing CRD	{"crd": "rayclusters.ray.io"}
2023-12-13T19:42:44Z	DEBUG	controller-runtime.test-env	adding API in waitlist	{"GV": "ray.io/v1"}
2023-12-13T19:42:44Z	DEBUG	controller-runtime.test-env	adding API in waitlist	{"GV": "ray.io/v1alpha1"}
2023-12-13T19:42:44Z	DEBUG	controller-runtime.test-env	adding API in waitlist	{"GV": "ray.io/v1"}
2023-12-13T19:42:44Z	DEBUG	controller-runtime.test-env	adding API in waitlist	{"GV": "ray.io/v1alpha1"}
2023-12-13T19:42:44Z	DEBUG	controller-runtime.test-env	adding API in waitlist	{"GV": "ray.io/v1"}
2023-12-13T19:42:44Z	DEBUG	controller-runtime.test-env	adding API in waitlist	{"GV": "ray.io/v1alpha1"}
2023-12-13T19:42:47Z	DEBUG	controller-runtime.test-env	installing webhooks
2023-12-13T19:42:47Z	INFO	controllers.RayCluster	Starting Reconciler
2023-12-13T19:42:47Z	INFO	Starting EventSource	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "source": "kind source: *v1.RayCluster"}
2023-12-13T19:42:47Z	INFO	Starting EventSource	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "source": "kind source: *v1.Pod"}
2023-12-13T19:42:47Z	INFO	Starting EventSource	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "source": "kind source: *v1.Service"}
2023-12-13T19:42:47Z	INFO	Starting Controller	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster"}
2023-12-13T19:42:47Z	INFO	Starting EventSource	{"controller": "rayjob", "controllerGroup": "ray.io", "controllerKind": "RayJob", "source": "kind source: *v1.RayJob"}
2023-12-13T19:42:47Z	INFO	Starting EventSource	{"controller": "rayjob", "controllerGroup": "ray.io", "controllerKind": "RayJob", "source": "kind source: *v1.RayCluster"}
2023-12-13T19:42:47Z	INFO	Starting EventSource	{"controller": "rayjob", "controllerGroup": "ray.io", "controllerKind": "RayJob", "source": "kind source: *v1.Service"}
2023-12-13T19:42:47Z	INFO	Starting EventSource	{"controller": "rayjob", "controllerGroup": "ray.io", "controllerKind": "RayJob", "source": "kind source: *v1.Job"}
2023-12-13T19:42:47Z	INFO	Starting Controller	{"controller": "rayjob", "controllerGroup": "ray.io", "controllerKind": "RayJob"}
2023-12-13T19:42:47Z	INFO	Starting EventSource	{"controller": "rayservice", "controllerGroup": "ray.io", "controllerKind": "RayService", "source": "kind source: *v1.RayService"}
2023-12-13T19:42:47Z	INFO	Starting EventSource	{"controller": "rayservice", "controllerGroup": "ray.io", "controllerKind": "RayService", "source": "kind source: *v1.RayCluster"}
2023-12-13T19:42:47Z	INFO	Starting EventSource	{"controller": "rayservice", "controllerGroup": "ray.io", "controllerKind": "RayService", "source": "kind source: *v1.Service"}
2023-12-13T19:42:47Z	INFO	Starting EventSource	{"controller": "rayservice", "controllerGroup": "ray.io", "controllerKind": "RayService", "source": "kind source: *v1.Ingress"}
2023-12-13T19:42:47Z	INFO	Starting Controller	{"controller": "rayservice", "controllerGroup": "ray.io", "controllerKind": "RayService"}
2023-12-13T19:42:48Z	INFO	Starting workers	{"controller": "rayservice", "controllerGroup": "ray.io", "controllerKind": "RayService", "worker count": 1}
2023-12-13T19:42:48Z	INFO	Starting workers	{"controller": "rayjob", "controllerGroup": "ray.io", "controllerKind": "RayJob", "worker count": 1}
2023-12-13T19:42:48Z	INFO	Starting workers	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "worker count": 1}
[38;5;10mâ€¢[0m2023-12-13T19:42:49Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
[38;5;10mâ€¢[0m2023-12-13T19:42:49Z	INFO	controllers.RayCluster	Pod ServiceAccount created successfully	{"service account name": "raycluster-sample"}
2023-12-13T19:42:49Z	DEBUG	events	Created service account raycluster-sample	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"210"}, "reason": "Created"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Role created successfully	{"role name": "raycluster-sample"}
2023-12-13T19:42:50Z	DEBUG	events	Created role raycluster-sample	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"210"}, "reason": "Created"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	RoleBinding created successfully	{"role binding name": "raycluster-sample"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:50Z	DEBUG	events	Created role binding raycluster-sample	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"210"}, "reason": "Created"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Pod Service created successfully	{"service name": "raycluster-sample-head-svc"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found 0 head Pods; creating a head Pod for the RayCluster.": "raycluster-sample"}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	head pod labels	{"labels": {"app.kubernetes.io/created-by":"kuberay-operator","app.kubernetes.io/name":"kuberay","ray.io/cluster":"raycluster-sample","ray.io/group":"headgroup","ray.io/identifier":"raycluster-sample-head","ray.io/is-ray-node":"yes","ray.io/node-type":"head"}}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "head", "rayStartParams": {"block":"true","dashboard-host":"0.0.0.0","metrics-export-port":"8080","no-monitor":"true","node-manager-port":"12346","num-cpus":"1","object-manager-port":"12345","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start --head  --metrics-export-port=8080  --block  --no-monitor  --object-manager-port=12345  --port=6379  --node-manager-port=12346  --num-cpus=1  --dashboard-host=0.0.0.0 "}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "head", "generatedCmd": "ulimit -n 65536; ray start --head  --metrics-export-port=8080  --block  --no-monitor  --object-manager-port=12345  --port=6379  --node-manager-port=12346  --num-cpus=1  --dashboard-host=0.0.0.0 "}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	createHeadPod	{"head pod with name": "raycluster-sample-head-"}
2023-12-13T19:42:50Z	DEBUG	events	Created service raycluster-sample-head-svc	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"210"}, "reason": "Created"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 3, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 3}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 0, "worker group": "small-group"}
2023-12-13T19:42:50Z	DEBUG	events	Created head pod raycluster-sample-head-rnbxm	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"210"}, "reason": "Created"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 0, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 3, "runningPods": 0, "diff": 3}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Number workers to add": 3, "Worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 0": "in total 3"}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --metrics-export-port=8080  --block  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379 "}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --metrics-export-port=8080  --block  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379 "}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 1": "in total 3"}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:42:50Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"210"}, "reason": "Created"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 2": "in total 3"}
2023-12-13T19:42:50Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"210"}, "reason": "Created"}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
time="2023-12-13T19:42:50Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-wqfsw; Pod Status.Phase: Pending"
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	inconsistentRayClusterStatus	{"detect inconsistency": "old AvailableWorkerReplicas: 0, new AvailableWorkerReplicas: 0, old DesiredWorkerReplicas: 0, new DesiredWorkerReplicas: 3, old MinWorkerReplicas: 0, new MinWorkerReplicas: 0, old MaxWorkerReplicas: 0, new MaxWorkerReplicas: 4"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	rayClusterReconcile	{"Update CR status": "raycluster-sample", "status": {"desiredWorkerReplicas":3,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:50Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":1}}
2023-12-13T19:42:50Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"210"}, "reason": "Created"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 3, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 3}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-swn66", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-swn66. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-pb7rf", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-pb7rf. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 3, "runningPods": 3, "diff": 0}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
time="2023-12-13T19:42:50Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-head-rnbxm; Pod Status.Phase: Pending"
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
[38;5;10mâ€¢[0m[38;5;10mâ€¢[0m[38;5;10mâ€¢[0m[38;5;10mâ€¢[0m2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 3, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 3}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-swn66", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-swn66. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-pb7rf", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-pb7rf. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 3, "runningPods": 3, "diff": 0}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
time="2023-12-13T19:42:50Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-pb7rf; Pod Status.Phase: Pending"
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 3, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 3}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-swn66", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-swn66. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-pb7rf", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-pb7rf. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 3, "runningPods": 3, "diff": 0}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
time="2023-12-13T19:42:50Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-wqfsw; Pod Status.Phase: Pending"
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	inconsistentRayClusterStatus	{"detect inconsistency": "old AvailableWorkerReplicas: 0, new AvailableWorkerReplicas: 1, old DesiredWorkerReplicas: 3, new DesiredWorkerReplicas: 3, old MinWorkerReplicas: 0, new MinWorkerReplicas: 0, old MaxWorkerReplicas: 4, new MaxWorkerReplicas: 4"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	rayClusterReconcile	{"Update CR status": "raycluster-sample", "status": {"availableWorkerReplicas":1,"desiredWorkerReplicas":3,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:50Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":1}}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 3, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 3}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-swn66", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-swn66. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-pb7rf", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-pb7rf. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 3, "runningPods": 3, "diff": 0}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
time="2023-12-13T19:42:50Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-wqfsw; Pod Status.Phase: Pending"
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	inconsistentRayClusterStatus	{"detect inconsistency": "old AvailableWorkerReplicas: 0, new AvailableWorkerReplicas: 2, old DesiredWorkerReplicas: 3, new DesiredWorkerReplicas: 3, old MinWorkerReplicas: 0, new MinWorkerReplicas: 0, old MaxWorkerReplicas: 4, new MaxWorkerReplicas: 4"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	rayClusterReconcile	{"Update CR status": "raycluster-sample", "status": {"availableWorkerReplicas":2,"desiredWorkerReplicas":3,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:50Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":1}}
[38;5;10mâ€¢[0m2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Got error when updating status	{"cluster name": "raycluster-sample", "error": "Operation cannot be fulfilled on rayclusters.ray.io \"raycluster-sample\": the object has been modified; please apply your changes to the latest version and try again", "RayCluster": {"kind":"RayCluster","apiVersion":"ray.io/v1","metadata":{"name":"raycluster-sample","namespace":"default","uid":"88165eec-5900-4c38-a006-570b61e22a0b","resourceVersion":"227","generation":1,"creationTimestamp":"2023-12-13T19:42:49Z","managedFields":[{"manager":"ray.test","operation":"Update","apiVersion":"ray.io/v1","time":"2023-12-13T19:42:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{".":{},"f:enableInTreeAutoscaling":{},"f:headGroupSpec":{".":{},"f:rayStartParams":{".":{},"f:node-manager-port":{},"f:num-cpus":{},"f:object-manager-port":{},"f:port":{}},"f:template":{".":{},"f:metadata":{},"f:spec":{".":{},"f:containers":{}}}},"f:workerGroupSpecs":{}}}},{"manager":"ray.test","operation":"Update","apiVersion":"ray.io/v1","time":"2023-12-13T19:42:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{".":{},"f:desiredWorkerReplicas":{},"f:endpoints":{".":{},"f:client":{},"f:dashboard":{},"f:metrics":{},"f:redis":{},"f:serve":{}},"f:head":{".":{},"f:serviceIP":{}},"f:lastUpdateTime":{},"f:maxWorkerReplicas":{},"f:observedGeneration":{}}},"subresource":"status"}]},"spec":{"headGroupSpec":{"rayStartParams":{"node-manager-port":"12346","num-cpus":"1","object-manager-port":"12345","port":"6379"},"template":{"metadata":{"creationTimestamp":null},"spec":{"containers":[{"name":"ray-head","image":"rayproject/ray:2.8.0","command":["python"],"args":["/opt/code.py"],"env":[{"name":"MY_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"resources":{}}]}}},"workerGroupSpecs":[{"groupName":"small-group","replicas":3,"minReplicas":0,"maxReplicas":4,"rayStartParams":{"num-cpus":"1","port":"6379"},"template":{"metadata":{"creationTimestamp":null},"spec":{"containers":[{"name":"ray-worker","image":"rayproject/ray:2.8.0","command":["echo"],"args":["Hello Ray"],"env":[{"name":"MY_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"resources":{}}]}},"scaleStrategy":{}}],"enableInTreeAutoscaling":true},"status":{"availableWorkerReplicas":2,"desiredWorkerReplicas":3,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:50Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":1}}}
2023-12-13T19:42:50Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "7d53035b-f091-4a22-aa78-4ce293f32314"}
2023-12-13T19:42:50Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "7d53035b-f091-4a22-aa78-4ce293f32314", "error": "Operation cannot be fulfilled on rayclusters.ray.io \"raycluster-sample\": the object has been modified; please apply your changes to the latest version and try again"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 3, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 3}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-swn66", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-swn66. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-pb7rf", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-pb7rf. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 3, "runningPods": 3, "diff": 0}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	inconsistentRayClusterStatus	{"detect inconsistency": "old State: , new State: ready, old Reason: , new Reason: "}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	rayClusterReconcile	{"Update CR status": "raycluster-sample", "status": {"state":"ready","availableWorkerReplicas":3,"desiredWorkerReplicas":3,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:50Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":1}}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 3, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 3}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-swn66", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-swn66. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-pb7rf", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-pb7rf. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 3, "runningPods": 3, "diff": 0}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
[38;5;10mâ€¢[0m2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 3, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 3}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-swn66", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-swn66. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-pb7rf", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-pb7rf. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 3, "runningPods": 3, "diff": 0}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	inconsistentRayClusterStatus	{"detect inconsistency": "old AvailableWorkerReplicas: 3, new AvailableWorkerReplicas: 2, old DesiredWorkerReplicas: 3, new DesiredWorkerReplicas: 3, old MinWorkerReplicas: 0, new MinWorkerReplicas: 0, old MaxWorkerReplicas: 4, new MaxWorkerReplicas: 4"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	rayClusterReconcile	{"Update CR status": "raycluster-sample", "status": {"state":"ready","availableWorkerReplicas":2,"desiredWorkerReplicas":3,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:50Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":1}}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 3, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 3}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 2, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 2, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-swn66", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-swn66. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 3, "runningPods": 2, "diff": 1}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Number workers to add": 1, "Worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 0": "in total 1"}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:42:50Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
time="2023-12-13T19:42:50Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-vlp2w; Pod Status.Phase: Pending"
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 3, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 3}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"238"}, "reason": "Created"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 3, "worker group": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-swn66", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-swn66. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-vlp2w", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-vlp2w. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 3, "runningPods": 3, "diff": 0}
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
time="2023-12-13T19:42:50Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-vlp2w; Pod Status.Phase: Pending"
2023-12-13T19:42:50Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
[38;5;10mâ€¢[0m[38;5;10mâ€¢[0m2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 2, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 2}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 3, "worker group": "small-group"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 3, "worker group": "small-group"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-swn66", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-swn66. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-vlp2w", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-vlp2w. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 2, "runningPods": 3, "diff": -1}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	Random Pod deletion is disabled for cluster raycluster-sample. The only decision-maker for Pod deletions is Autoscaler.
time="2023-12-13T19:42:51Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-vlp2w; Pod Status.Phase: Pending"
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	inconsistentRayClusterStatus	{"detect inconsistency": "old AvailableWorkerReplicas: 2, new AvailableWorkerReplicas: 2, old DesiredWorkerReplicas: 3, new DesiredWorkerReplicas: 2, old MinWorkerReplicas: 0, new MinWorkerReplicas: 0, old MaxWorkerReplicas: 4, new MaxWorkerReplicas: 4"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	rayClusterReconcile	{"Update CR status": "raycluster-sample", "status": {"state":"ready","availableWorkerReplicas":2,"desiredWorkerReplicas":2,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:51Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":2}}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	Got error when updating status	{"cluster name": "raycluster-sample", "error": "Operation cannot be fulfilled on rayclusters.ray.io \"raycluster-sample\": the object has been modified; please apply your changes to the latest version and try again", "RayCluster": {"kind":"RayCluster","apiVersion":"ray.io/v1","metadata":{"name":"raycluster-sample","namespace":"default","uid":"88165eec-5900-4c38-a006-570b61e22a0b","resourceVersion":"241","generation":2,"creationTimestamp":"2023-12-13T19:42:49Z","managedFields":[{"manager":"ray.test","operation":"Update","apiVersion":"ray.io/v1","time":"2023-12-13T19:42:49Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{".":{},"f:enableInTreeAutoscaling":{},"f:headGroupSpec":{".":{},"f:rayStartParams":{".":{},"f:node-manager-port":{},"f:num-cpus":{},"f:object-manager-port":{},"f:port":{}},"f:template":{".":{},"f:metadata":{},"f:spec":{".":{},"f:containers":{}}}},"f:workerGroupSpecs":{}}}},{"manager":"ray.test","operation":"Update","apiVersion":"ray.io/v1","time":"2023-12-13T19:42:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{".":{},"f:availableWorkerReplicas":{},"f:desiredWorkerReplicas":{},"f:endpoints":{".":{},"f:client":{},"f:dashboard":{},"f:metrics":{},"f:redis":{},"f:serve":{}},"f:head":{".":{},"f:serviceIP":{}},"f:lastUpdateTime":{},"f:maxWorkerReplicas":{},"f:observedGeneration":{},"f:state":{}}},"subresource":"status"}]},"spec":{"headGroupSpec":{"rayStartParams":{"node-manager-port":"12346","num-cpus":"1","object-manager-port":"12345","port":"6379"},"template":{"metadata":{"creationTimestamp":null},"spec":{"containers":[{"name":"ray-head","image":"rayproject/ray:2.8.0","command":["python"],"args":["/opt/code.py"],"env":[{"name":"MY_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"resources":{}}]}}},"workerGroupSpecs":[{"groupName":"small-group","replicas":2,"minReplicas":0,"maxReplicas":4,"rayStartParams":{"num-cpus":"1","port":"6379"},"template":{"metadata":{"creationTimestamp":null},"spec":{"containers":[{"name":"ray-worker","image":"rayproject/ray:2.8.0","command":["echo"],"args":["Hello Ray"],"env":[{"name":"MY_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"resources":{}}]}},"scaleStrategy":{}}],"enableInTreeAutoscaling":true},"status":{"state":"ready","availableWorkerReplicas":2,"desiredWorkerReplicas":2,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:51Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":2}}}
2023-12-13T19:42:51Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "c6e3d532-7f72-4cb3-8722-f15594f00e4f"}
2023-12-13T19:42:51Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "c6e3d532-7f72-4cb3-8722-f15594f00e4f", "error": "Operation cannot be fulfilled on rayclusters.ray.io \"raycluster-sample\": the object has been modified; please apply your changes to the latest version and try again"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 2, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 2}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 3, "worker group": "small-group"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 3, "worker group": "small-group"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-swn66", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-swn66. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-vlp2w", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-vlp2w. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 2, "runningPods": 2, "diff": 0}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
2023-12-13T19:42:51Z	DEBUG	events	Deleted pod raycluster-sample-worker-small-group-swn66	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"242"}, "reason": "Deleted"}
time="2023-12-13T19:42:51Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-vlp2w; Pod Status.Phase: Pending"
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	inconsistentRayClusterStatus	{"detect inconsistency": "old AvailableWorkerReplicas: 2, new AvailableWorkerReplicas: 1, old DesiredWorkerReplicas: 3, new DesiredWorkerReplicas: 2, old MinWorkerReplicas: 0, new MinWorkerReplicas: 0, old MaxWorkerReplicas: 4, new MaxWorkerReplicas: 4"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	rayClusterReconcile	{"Update CR status": "raycluster-sample", "status": {"state":"ready","availableWorkerReplicas":1,"desiredWorkerReplicas":2,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:51Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":3}}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 2, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 2}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 2, "worker group": "small-group"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 2, "worker group": "small-group"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-vlp2w", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-vlp2w. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 2, "runningPods": 2, "diff": 0}
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
time="2023-12-13T19:42:51Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-vlp2w; Pod Status.Phase: Pending"
2023-12-13T19:42:51Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
[38;5;10mâ€¢[0m[38;5;10mâ€¢[0m2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
time="2023-12-13T19:42:52Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 2, "worker group": "small-group"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 2, "worker group": "small-group"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-vlp2w", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-vlp2w. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 2, "diff": 2}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"Number workers to add": 2, "Worker group": "small-group"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 0": "in total 2"}
2023-12-13T19:42:52Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:42:52Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:42:52Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --block  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080 "}
2023-12-13T19:42:52Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --block  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080 "}
2023-12-13T19:42:52Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 1": "in total 2"}
2023-12-13T19:42:52Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:42:52Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:42:52Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:42:52Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:42:52Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:42:52Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"247"}, "reason": "Created"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
time="2023-12-13T19:42:52Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:42:52Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-xt687; Pod Status.Phase: Pending"
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	inconsistentRayClusterStatus	{"detect inconsistency": "old AvailableWorkerReplicas: 1, new AvailableWorkerReplicas: 1, old DesiredWorkerReplicas: 2, new DesiredWorkerReplicas: 4, old MinWorkerReplicas: 0, new MinWorkerReplicas: 0, old MaxWorkerReplicas: 4, new MaxWorkerReplicas: 4"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	rayClusterReconcile	{"Update CR status": "raycluster-sample", "status": {"state":"ready","availableWorkerReplicas":1,"desiredWorkerReplicas":4,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:52Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":4}}
2023-12-13T19:42:52Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"247"}, "reason": "Created"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-rnbxm", "Pod status": "Running", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-rnbxm", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-rnbxm. The Pod status is Running, and the Ray container terminated status is nil."}
time="2023-12-13T19:42:52Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 4, "worker group": "small-group"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 4, "worker group": "small-group"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-xt687", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-xt687. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-wqfsw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-wqfsw. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-vlp2w", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-vlp2w. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-vmlll", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-vmlll. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 4, "diff": 0}
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
time="2023-12-13T19:42:52Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:42:52Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-xt687; Pod Status.Phase: Pending"
2023-12-13T19:42:52Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
[38;5;10mâ€¢[0m[38;5;10mâ€¢[0m2023-12-13T19:42:54Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:54Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:54Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
time="2023-12-13T19:42:54Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:42:54Z	INFO	controllers.RayCluster	Found 0 head pods. cluster name raycluster-sample, filter labels map[ray.io/cluster:raycluster-sample ray.io/node-type:head]
2023-12-13T19:42:54Z	INFO	controllers.RayCluster	inconsistentRayClusterStatus	{"detect inconsistency": "old State: ready, new State: suspended, old Reason: , new Reason: "}
2023-12-13T19:42:54Z	INFO	controllers.RayCluster	rayClusterReconcile	{"Update CR status": "raycluster-sample", "status": {"state":"suspended","desiredWorkerReplicas":4,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:54Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":5}}
2023-12-13T19:42:54Z	DEBUG	events	Deleted Pods for RayCluster default/raycluster-sample due to suspension	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"253"}, "reason": "Deleted"}
2023-12-13T19:42:54Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:54Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:54Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:54Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
time="2023-12-13T19:42:54Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:42:54Z	DEBUG	events	Deleted Pods for RayCluster default/raycluster-sample due to suspension	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"265"}, "reason": "Deleted"}
2023-12-13T19:42:54Z	INFO	controllers.RayCluster	Found 0 head pods. cluster name raycluster-sample, filter labels map[ray.io/cluster:raycluster-sample ray.io/node-type:head]
2023-12-13T19:42:54Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
[38;5;10mâ€¢[0m[38;5;10mâ€¢[0m2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"Found 0 head Pods; creating a head Pod for the RayCluster.": "raycluster-sample"}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	head pod labels	{"labels": {"app.kubernetes.io/created-by":"kuberay-operator","app.kubernetes.io/name":"kuberay","ray.io/cluster":"raycluster-sample","ray.io/group":"headgroup","ray.io/identifier":"raycluster-sample-head","ray.io/is-ray-node":"yes","ray.io/node-type":"head"}}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "head", "rayStartParams": {"block":"true","dashboard-host":"0.0.0.0","metrics-export-port":"8080","no-monitor":"true","node-manager-port":"12346","num-cpus":"1","object-manager-port":"12345","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start --head  --node-manager-port=12346  --num-cpus=1  --object-manager-port=12345  --port=6379  --dashboard-host=0.0.0.0  --metrics-export-port=8080  --block  --no-monitor "}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "head", "generatedCmd": "ulimit -n 65536; ray start --head  --node-manager-port=12346  --num-cpus=1  --object-manager-port=12345  --port=6379  --dashboard-host=0.0.0.0  --metrics-export-port=8080  --block  --no-monitor "}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	createHeadPod	{"head pod with name": "raycluster-sample-head-"}
time="2023-12-13T19:42:57Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 0, "worker group": "small-group"}
2023-12-13T19:42:57Z	DEBUG	events	Created head pod raycluster-sample-head-4ljmp	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"267"}, "reason": "Created"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 0, "worker group": "small-group"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 0, "diff": 4}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"Number workers to add": 4, "Worker group": "small-group"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 0": "in total 4"}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 1": "in total 4"}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:42:57Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"267"}, "reason": "Created"}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 2": "in total 4"}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --block  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080 "}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --block  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080 "}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:42:57Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"267"}, "reason": "Created"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 3": "in total 4"}
2023-12-13T19:42:57Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"267"}, "reason": "Created"}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block  --num-cpus=1  --port=6379 "}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block  --num-cpus=1  --port=6379 "}
2023-12-13T19:42:57Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
time="2023-12-13T19:42:57Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:42:57Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-head-4ljmp; Pod Status.Phase: Pending"
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-4ljmp", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-4ljmp", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-4ljmp. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:42:57Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 4, "worker group": "small-group"}
2023-12-13T19:42:57Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"267"}, "reason": "Created"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 4, "worker group": "small-group"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-5nczd", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-5nczd. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-xzxcv", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-xzxcv. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-h6th9", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-h6th9. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-xmstz", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-xmstz. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 4, "diff": 0}
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
time="2023-12-13T19:42:57Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:42:57Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-5nczd; Pod Status.Phase: Pending"
2023-12-13T19:42:57Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-4ljmp", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-4ljmp", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-4ljmp. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:42:59Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 4, "worker group": "small-group"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 4, "worker group": "small-group"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-xmstz", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-xmstz. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-5nczd", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-5nczd. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-xzxcv", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-xzxcv. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-h6th9", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-h6th9. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 4, "diff": 0}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
time="2023-12-13T19:42:59Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:42:59Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-head-4ljmp; Pod Status.Phase: Pending"
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	inconsistentRayClusterStatus	{"detect inconsistency": "old AvailableWorkerReplicas: 0, new AvailableWorkerReplicas: 1, old DesiredWorkerReplicas: 4, new DesiredWorkerReplicas: 4, old MinWorkerReplicas: 0, new MinWorkerReplicas: 0, old MaxWorkerReplicas: 4, new MaxWorkerReplicas: 4"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	rayClusterReconcile	{"Update CR status": "raycluster-sample", "status": {"state":"suspended","availableWorkerReplicas":1,"desiredWorkerReplicas":4,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:59Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":6}}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-4ljmp", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-4ljmp", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-4ljmp. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:42:59Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 4, "worker group": "small-group"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 4, "worker group": "small-group"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-h6th9", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-h6th9. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-xmstz", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-xmstz. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-5nczd", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-5nczd. The Pod status is Running, and the Ray container terminated status is nil."}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-xzxcv", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-xzxcv. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 4, "diff": 0}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
time="2023-12-13T19:42:59Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:42:59Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-xzxcv; Pod Status.Phase: Pending"
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	inconsistentRayClusterStatus	{"detect inconsistency": "old AvailableWorkerReplicas: 1, new AvailableWorkerReplicas: 3, old DesiredWorkerReplicas: 4, new DesiredWorkerReplicas: 4, old MinWorkerReplicas: 0, new MinWorkerReplicas: 0, old MaxWorkerReplicas: 4, new MaxWorkerReplicas: 4"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	rayClusterReconcile	{"Update CR status": "raycluster-sample", "status": {"state":"suspended","availableWorkerReplicas":3,"desiredWorkerReplicas":4,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:59Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":6}}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	Got error when updating status	{"cluster name": "raycluster-sample", "error": "Operation cannot be fulfilled on rayclusters.ray.io \"raycluster-sample\": the object has been modified; please apply your changes to the latest version and try again", "RayCluster": {"kind":"RayCluster","apiVersion":"ray.io/v1","metadata":{"name":"raycluster-sample","namespace":"default","uid":"88165eec-5900-4c38-a006-570b61e22a0b","resourceVersion":"280","generation":6,"creationTimestamp":"2023-12-13T19:42:49Z","managedFields":[{"manager":"ray.test","operation":"Update","apiVersion":"ray.io/v1","time":"2023-12-13T19:42:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{".":{},"f:enableInTreeAutoscaling":{},"f:headGroupSpec":{".":{},"f:rayStartParams":{".":{},"f:node-manager-port":{},"f:num-cpus":{},"f:object-manager-port":{},"f:port":{}},"f:template":{".":{},"f:metadata":{},"f:spec":{".":{},"f:containers":{}}}},"f:suspend":{},"f:workerGroupSpecs":{}}}},{"manager":"ray.test","operation":"Update","apiVersion":"ray.io/v1","time":"2023-12-13T19:42:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{".":{},"f:availableWorkerReplicas":{},"f:desiredWorkerReplicas":{},"f:endpoints":{".":{},"f:client":{},"f:dashboard":{},"f:metrics":{},"f:redis":{},"f:serve":{}},"f:head":{".":{},"f:serviceIP":{}},"f:lastUpdateTime":{},"f:maxWorkerReplicas":{},"f:observedGeneration":{},"f:state":{}}},"subresource":"status"}]},"spec":{"headGroupSpec":{"rayStartParams":{"node-manager-port":"12346","num-cpus":"1","object-manager-port":"12345","port":"6379"},"template":{"metadata":{"creationTimestamp":null},"spec":{"containers":[{"name":"ray-head","image":"rayproject/ray:2.8.0","command":["python"],"args":["/opt/code.py"],"env":[{"name":"MY_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"resources":{}}]}}},"workerGroupSpecs":[{"groupName":"small-group","replicas":5,"minReplicas":0,"maxReplicas":4,"rayStartParams":{"num-cpus":"1","port":"6379"},"template":{"metadata":{"creationTimestamp":null},"spec":{"containers":[{"name":"ray-worker","image":"rayproject/ray:2.8.0","command":["echo"],"args":["Hello Ray"],"env":[{"name":"MY_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"resources":{}}]}},"scaleStrategy":{"workersToDelete":["raycluster-sample-worker-small-group-swn66"]}}],"enableInTreeAutoscaling":true,"suspend":false},"status":{"state":"suspended","availableWorkerReplicas":3,"desiredWorkerReplicas":4,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:59Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":6}}}
2023-12-13T19:42:59Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "85d35f3a-cd07-4531-88fc-2144708697de"}
2023-12-13T19:42:59Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "85d35f3a-cd07-4531-88fc-2144708697de", "error": "Operation cannot be fulfilled on rayclusters.ray.io \"raycluster-sample\": the object has been modified; please apply your changes to the latest version and try again"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
time="2023-12-13T19:42:59Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	Found 0 head pods. cluster name raycluster-sample, filter labels map[ray.io/cluster:raycluster-sample ray.io/node-type:head]
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	inconsistentRayClusterStatus	{"detect inconsistency": "old AvailableWorkerReplicas: 1, new AvailableWorkerReplicas: 0, old DesiredWorkerReplicas: 4, new DesiredWorkerReplicas: 4, old MinWorkerReplicas: 0, new MinWorkerReplicas: 0, old MaxWorkerReplicas: 4, new MaxWorkerReplicas: 4"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	rayClusterReconcile	{"Update CR status": "raycluster-sample", "status": {"state":"suspended","desiredWorkerReplicas":4,"maxWorkerReplicas":4,"lastUpdateTime":"2023-12-13T19:42:59Z","endpoints":{"client":"10001","dashboard":"8265","metrics":"8080","redis":"6379","serve":"8000"},"head":{"serviceIP":"10.0.0.10"},"observedGeneration":7}}
2023-12-13T19:42:59Z	DEBUG	events	Deleted Pods for RayCluster default/raycluster-sample due to suspension	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"283"}, "reason": "Deleted"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
time="2023-12-13T19:42:59Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	Found 0 head pods. cluster name raycluster-sample, filter labels map[ray.io/cluster:raycluster-sample ray.io/node-type:head]
2023-12-13T19:42:59Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:42:59Z	DEBUG	events	Deleted Pods for RayCluster default/raycluster-sample due to suspension	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"295"}, "reason": "Deleted"}
[38;5;10mâ€¢[0m2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Found 0 head Pods; creating a head Pod for the RayCluster.": "raycluster-sample"}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	head pod labels	{"labels": {"app.kubernetes.io/created-by":"kuberay-operator","app.kubernetes.io/name":"kuberay","ray.io/cluster":"raycluster-sample","ray.io/group":"headgroup","ray.io/identifier":"raycluster-sample-head","ray.io/is-ray-node":"yes","ray.io/node-type":"head"}}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "head", "rayStartParams": {"block":"true","dashboard-host":"0.0.0.0","metrics-export-port":"8080","no-monitor":"true","node-manager-port":"12346","num-cpus":"1","object-manager-port":"12345","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start --head  --node-manager-port=12346  --num-cpus=1  --object-manager-port=12345  --port=6379  --dashboard-host=0.0.0.0  --metrics-export-port=8080  --block  --no-monitor "}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "head", "generatedCmd": "ulimit -n 65536; ray start --head  --node-manager-port=12346  --num-cpus=1  --object-manager-port=12345  --port=6379  --dashboard-host=0.0.0.0  --metrics-export-port=8080  --block  --no-monitor "}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	createHeadPod	{"head pod with name": "raycluster-sample-head-"}
time="2023-12-13T19:43:02Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 0, "worker group": "small-group"}
2023-12-13T19:43:02Z	DEBUG	events	Created head pod raycluster-sample-head-qrll9	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "Created"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 0, "worker group": "small-group"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 0, "diff": 4}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Number workers to add": 4, "Worker group": "small-group"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 0": "in total 4"}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 1": "in total 4"}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:43:02Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "Created"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 2": "in total 4"}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block  --num-cpus=1  --port=6379 "}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block  --num-cpus=1  --port=6379 "}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:43:02Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "Created"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 3": "in total 4"}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --num-cpus=1  --port=6379  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080  --block "}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:43:02Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "Created"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
time="2023-12-13T19:43:02Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:43:02Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-head-qrll9; Pod Status.Phase: Pending"
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:02Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 3, "worker group": "small-group"}
2023-12-13T19:43:02Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "Created"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 4, "worker group": "small-group"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-j56m2", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-j56m2. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-tlh2d", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-tlh2d. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-cgxpd", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-cgxpd. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 3, "diff": 1}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Number workers to add": 1, "Worker group": "small-group"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"creating worker for group": "small-group", "index 0": "in total 1"}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	Setting pod namespaces	{"namespace": "default"}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	generateRayStartCommand	{"nodeType": "worker", "rayStartParams": {"address":"raycluster-sample-head-svc.default.svc.cluster.local:6379","block":"true","metrics-export-port":"8080","num-cpus":"1","port":"6379"}, "Ray container resource": {}}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	generateRayStartCommand	{"rayStartCmd": "ray start  --block  --port=6379  --num-cpus=1  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080 "}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	BuildPod	{"rayNodeType": "worker", "generatedCmd": "ulimit -n 65536; ray start  --block  --port=6379  --num-cpus=1  --address=raycluster-sample-head-svc.default.svc.cluster.local:6379  --metrics-export-port=8080 "}
2023-12-13T19:43:02Z	INFO	RayCluster-Controller	Probes injection feature flag	{"enabled": true}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Created pod	{"Pod ": "raycluster-sample-worker-small-group-"}
time="2023-12-13T19:43:02Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:43:02Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-head-qrll9; Pod Status.Phase: Pending"
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:02Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 4, "worker group": "small-group"}
2023-12-13T19:43:02Z	DEBUG	events	Created worker pod 	{"type": "Normal", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "Created"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 5, "worker group": "small-group"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-j56m2", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-j56m2. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-tlh2d", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-tlh2d. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-cgxpd", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-cgxpd. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-tc4w7", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-tc4w7. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 4, "diff": 0}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"all workers already exist for group": "small-group"}
time="2023-12-13T19:43:02Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:43:02Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-head-qrll9; Pod Status.Phase: Pending"
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:02Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 5, "worker group": "small-group"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-cgxpd", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-cgxpd. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-tc4w7", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-tc4w7. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-85ngw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-85ngw. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-j56m2", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-j56m2. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-tlh2d", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-tlh2d. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 5, "diff": -1}
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Random Pod deletion is disabled for cluster raycluster-sample. The only decision-maker for Pod deletions is Autoscaler.
time="2023-12-13T19:43:02Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:43:02Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-tc4w7; Pod Status.Phase: Pending"
2023-12-13T19:43:02Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:09Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 5, "worker group": "small-group"}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-j56m2", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-j56m2. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-tlh2d", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-tlh2d. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-cgxpd", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-cgxpd. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-tc4w7", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-tc4w7. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-85ngw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-85ngw. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 5, "diff": -1}
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	Random Pod deletion is disabled for cluster raycluster-sample. The only decision-maker for Pod deletions is Autoscaler.
time="2023-12-13T19:43:09Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:43:09Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-85ngw; Pod Status.Phase: Pending"
2023-12-13T19:43:09Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}

[38;5;243m------------------------------[0m
[38;5;9mâ€¢ [FAILED] [15.536 seconds][0m
[0mInside the default namespace [38;5;243mWhen creating a raycluster [38;5;9m[1m[It] should run all head and worker pods if un-suspended[0m
[38;5;243m/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller_test.go:393[0m

  [38;5;9m[FAILED] Timed out after 15.003s.
  workerGroup []
  Expected
      <int>: 5
  to equal
      <int>: 4[0m
  [38;5;9mIn [1m[It][0m[38;5;9m at: [1m/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller_test.go:411[0m [38;5;243m@ 12/13/23 19:43:17.793[0m
[38;5;243m------------------------------[0m
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:19Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 5, "worker group": "small-group"}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-tlh2d", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-tlh2d. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-cgxpd", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-cgxpd. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-tc4w7", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-tc4w7. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-85ngw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-85ngw. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-j56m2", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-j56m2. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 5, "diff": -1}
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	Random Pod deletion is disabled for cluster raycluster-sample. The only decision-maker for Pod deletions is Autoscaler.
time="2023-12-13T19:43:19Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:43:19Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-j56m2; Pod Status.Phase: Pending"
2023-12-13T19:43:19Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:29Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods in k8s api server": 5, "worker group": "small-group"}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-j56m2", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-j56m2. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-tlh2d", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-tlh2d. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-cgxpd", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-cgxpd. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-tc4w7", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-tc4w7. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"worker Pod": "raycluster-sample-worker-small-group-85ngw", "shouldDelete": false, "reason": "KubeRay does not need to delete the worker Pod raycluster-sample-worker-small-group-85ngw. The Pod status is Pending, and the Ray container terminated status is nil."}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"removing the pods in the scaleStrategy of": "small-group"}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	Deleting pod	{"namespace": "default", "name": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"The worker Pod has already been deleted": "raycluster-sample-worker-small-group-swn66"}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	reconcilePods	{"workerReplicas": 4, "runningPods": 5, "diff": -1}
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	Random Pod deletion is disabled for cluster raycluster-sample. The only decision-maker for Pod deletions is Autoscaler.
time="2023-12-13T19:43:29Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:43:29Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-worker-small-group-tc4w7; Pod Status.Phase: Pending"
2023-12-13T19:43:29Z	INFO	controllers.RayCluster	Unconditional requeue after	{"cluster name": "raycluster-sample", "seconds": 10}
[38;5;9mâ€¢ [FAILED] [15.001 seconds][0m
[0mInside the default namespace [38;5;243mWhen creating a raycluster [38;5;9m[1m[It] cluster's .status.state should be updated back to 'ready' after being un-suspended[0m
[38;5;243m/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller_test.go:428[0m

  [38;5;9m[FAILED] Timed out after 15.001s.
  Expected
      <v1.ClusterState>: suspended
  to equal
      <v1.ClusterState>: ready[0m
  [38;5;9mIn [1m[It][0m[38;5;9m at: [1m/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller_test.go:431[0m [38;5;243m@ 12/13/23 19:43:32.797[0m
[38;5;243m------------------------------[0m
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:39Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	Fail to list worker Pods from K8s api server
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	updateClusterState	{"Update CR Status.State": "failed"}
2023-12-13T19:43:39Z	ERROR	controllers.RayCluster	RayCluster update state error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:357
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	updateClusterReason	{"Update CR Status.Reason": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
2023-12-13T19:43:39Z	ERROR	controllers.RayCluster	RayCluster update reason error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:360
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "aa131570-76ef-4b53-be5d-d071083a864b"}
2023-12-13T19:43:39Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "aa131570-76ef-4b53-be5d-d071083a864b", "error": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	DEBUG	events	Get "https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group": dial tcp 127.0.0.1:43751: connect: connection refused	{"type": "Warning", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "PodReconciliationError"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:39Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	Fail to list worker Pods from K8s api server
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	updateClusterState	{"Update CR Status.State": "failed"}
2023-12-13T19:43:39Z	ERROR	controllers.RayCluster	RayCluster update state error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:357
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	updateClusterReason	{"Update CR Status.Reason": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
2023-12-13T19:43:39Z	ERROR	controllers.RayCluster	RayCluster update reason error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:360
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "baf1c0e5-4421-47fa-9693-2b66c46217b3"}
2023-12-13T19:43:39Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "baf1c0e5-4421-47fa-9693-2b66c46217b3", "error": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	DEBUG	events	Get "https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group": dial tcp 127.0.0.1:43751: connect: connection refused	{"type": "Warning", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "PodReconciliationError"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:39Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	Fail to list worker Pods from K8s api server
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	updateClusterState	{"Update CR Status.State": "failed"}
2023-12-13T19:43:39Z	ERROR	controllers.RayCluster	RayCluster update state error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:357
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	updateClusterReason	{"Update CR Status.Reason": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
2023-12-13T19:43:39Z	ERROR	controllers.RayCluster	RayCluster update reason error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:360
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "6dae5480-fc9c-4339-ae87-1d27ca2568df"}
2023-12-13T19:43:39Z	DEBUG	events	Get "https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group": dial tcp 127.0.0.1:43751: connect: connection refused	{"type": "Warning", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "PodReconciliationError"}
2023-12-13T19:43:39Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "6dae5480-fc9c-4339-ae87-1d27ca2568df", "error": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:39Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	Fail to list worker Pods from K8s api server
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	updateClusterState	{"Update CR Status.State": "failed"}
2023-12-13T19:43:39Z	ERROR	controllers.RayCluster	RayCluster update state error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:357
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	updateClusterReason	{"Update CR Status.Reason": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
2023-12-13T19:43:39Z	ERROR	controllers.RayCluster	RayCluster update reason error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:360
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "bc0ef479-34d9-4ee9-8395-e96b87fcaa7d"}
2023-12-13T19:43:39Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "bc0ef479-34d9-4ee9-8395-e96b87fcaa7d", "error": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	DEBUG	events	Get "https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group": dial tcp 127.0.0.1:43751: connect: connection refused	{"type": "Warning", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "PodReconciliationError"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:39Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	Fail to list worker Pods from K8s api server
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	updateClusterState	{"Update CR Status.State": "failed"}
2023-12-13T19:43:39Z	ERROR	controllers.RayCluster	RayCluster update state error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:357
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	INFO	controllers.RayCluster	updateClusterReason	{"Update CR Status.Reason": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
2023-12-13T19:43:39Z	ERROR	controllers.RayCluster	RayCluster update reason error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:360
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "bc82c5c1-dbfd-4378-a5ea-a8203a28d05c"}
2023-12-13T19:43:39Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "bc82c5c1-dbfd-4378-a5ea-a8203a28d05c", "error": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:39Z	DEBUG	events	Get "https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group": dial tcp 127.0.0.1:43751: connect: connection refused	{"type": "Warning", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "PodReconciliationError"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:40Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	Fail to list worker Pods from K8s api server
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	updateClusterState	{"Update CR Status.State": "failed"}
2023-12-13T19:43:40Z	ERROR	controllers.RayCluster	RayCluster update state error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:357
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	updateClusterReason	{"Update CR Status.Reason": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
2023-12-13T19:43:40Z	ERROR	controllers.RayCluster	RayCluster update reason error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:360
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:40Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "3b033232-4741-42ea-9392-a9512ea25c6b"}
2023-12-13T19:43:40Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "3b033232-4741-42ea-9392-a9512ea25c6b", "error": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:40Z	DEBUG	events	Get "https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group": dial tcp 127.0.0.1:43751: connect: connection refused	{"type": "Warning", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "PodReconciliationError"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:40Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	Fail to list worker Pods from K8s api server
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	updateClusterState	{"Update CR Status.State": "failed"}
2023-12-13T19:43:40Z	ERROR	controllers.RayCluster	RayCluster update state error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:357
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	updateClusterReason	{"Update CR Status.Reason": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
2023-12-13T19:43:40Z	ERROR	controllers.RayCluster	RayCluster update reason error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:360
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:40Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "a2ac5ad7-784e-4c44-9982-e2c9e8a57ab1"}
2023-12-13T19:43:40Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "a2ac5ad7-784e-4c44-9982-e2c9e8a57ab1", "error": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:40Z	DEBUG	events	Get "https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group": dial tcp 127.0.0.1:43751: connect: connection refused	{"type": "Warning", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "PodReconciliationError"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:40Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	Fail to list worker Pods from K8s api server
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	updateClusterState	{"Update CR Status.State": "failed"}
2023-12-13T19:43:40Z	ERROR	controllers.RayCluster	RayCluster update state error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:357
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:40Z	INFO	controllers.RayCluster	updateClusterReason	{"Update CR Status.Reason": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
2023-12-13T19:43:40Z	ERROR	controllers.RayCluster	RayCluster update reason error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:360
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:40Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "3362b5ce-54db-4ec2-8610-895bc58e9e88"}
2023-12-13T19:43:40Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "3362b5ce-54db-4ec2-8610-895bc58e9e88", "error": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:40Z	DEBUG	events	Get "https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group": dial tcp 127.0.0.1:43751: connect: connection refused	{"type": "Warning", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "PodReconciliationError"}
2023-12-13T19:43:41Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:41Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:41Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:41Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:41Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:41Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:41Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:41Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:41Z	INFO	controllers.RayCluster	Fail to list worker Pods from K8s api server
2023-12-13T19:43:41Z	INFO	controllers.RayCluster	updateClusterState	{"Update CR Status.State": "failed"}
2023-12-13T19:43:41Z	ERROR	controllers.RayCluster	RayCluster update state error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:357
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:41Z	INFO	controllers.RayCluster	updateClusterReason	{"Update CR Status.Reason": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
2023-12-13T19:43:41Z	ERROR	controllers.RayCluster	RayCluster update reason error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:360
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:41Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "cceae060-b906-45d2-93c9-4c7510e3a936"}
2023-12-13T19:43:41Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "cceae060-b906-45d2-93c9-4c7510e3a936", "error": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:41Z	DEBUG	events	Get "https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group": dial tcp 127.0.0.1:43751: connect: connection refused	{"type": "Warning", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "PodReconciliationError"}
2023-12-13T19:43:42Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:42Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:42Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:42Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:42Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:42Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:42Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:42Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:42Z	INFO	controllers.RayCluster	Fail to list worker Pods from K8s api server
2023-12-13T19:43:42Z	INFO	controllers.RayCluster	updateClusterState	{"Update CR Status.State": "failed"}
2023-12-13T19:43:42Z	ERROR	controllers.RayCluster	RayCluster update state error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:357
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:42Z	INFO	controllers.RayCluster	updateClusterReason	{"Update CR Status.Reason": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
2023-12-13T19:43:42Z	ERROR	controllers.RayCluster	RayCluster update reason error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:360
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:42Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "c03e9902-20d2-4c2d-8590-2eed5695b03d"}
2023-12-13T19:43:42Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "c03e9902-20d2-4c2d-8590-2eed5695b03d", "error": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:42Z	DEBUG	events	Get "https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group": dial tcp 127.0.0.1:43751: connect: connection refused	{"type": "Warning", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "PodReconciliationError"}
2023-12-13T19:43:45Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:45Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:45Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:45Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:45Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:45Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:45Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:45Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:45Z	INFO	controllers.RayCluster	Fail to list worker Pods from K8s api server
2023-12-13T19:43:45Z	INFO	controllers.RayCluster	updateClusterState	{"Update CR Status.State": "failed"}
2023-12-13T19:43:45Z	ERROR	controllers.RayCluster	RayCluster update state error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:357
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:45Z	INFO	controllers.RayCluster	updateClusterReason	{"Update CR Status.Reason": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
2023-12-13T19:43:45Z	ERROR	controllers.RayCluster	RayCluster update reason error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:360
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:45Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "73748e58-e8e6-4378-8890-e482d9bcd8b8"}
2023-12-13T19:43:45Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "73748e58-e8e6-4378-8890-e482d9bcd8b8", "error": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:45Z	DEBUG	events	Get "https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group": dial tcp 127.0.0.1:43751: connect: connection refused	{"type": "Warning", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "PodReconciliationError"}
2023-12-13T19:43:50Z	INFO	controllers.RayCluster	reconciling RayCluster	{"cluster name": "raycluster-sample"}
2023-12-13T19:43:50Z	INFO	controllers.RayCluster	Reconciling Ingress
2023-12-13T19:43:50Z	INFO	controllers.RayCluster	reconcileHeadService	{"1 head service found": "raycluster-sample-head-svc"}
2023-12-13T19:43:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found 1 head Pod": "raycluster-sample-head-qrll9", "Pod status": "Pending", "Pod restart policy": "Always", "Ray container terminated status": "nil"}
2023-12-13T19:43:50Z	INFO	controllers.RayCluster	reconcilePods	{"head Pod": "raycluster-sample-head-qrll9", "shouldDelete": false, "reason": "KubeRay does not need to delete the head Pod raycluster-sample-head-qrll9. The Pod status is Pending, and the Ray container terminated status is nil."}
time="2023-12-13T19:43:50Z" level=warning msg="replicas (5) is greater than maxReplicas (4), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
2023-12-13T19:43:50Z	INFO	controllers.RayCluster	reconcilePods	{"desired workerReplicas (always adhering to minReplicas/maxReplica)": 4, "worker group": "small-group", "maxReplicas": 4, "minReplicas": 0, "replicas": 5}
2023-12-13T19:43:50Z	INFO	controllers.RayCluster	reconcilePods	{"Found worker pods": 5, "worker group": "small-group"}
2023-12-13T19:43:50Z	INFO	controllers.RayCluster	Fail to list worker Pods from K8s api server
2023-12-13T19:43:50Z	INFO	controllers.RayCluster	updateClusterState	{"Update CR Status.State": "failed"}
2023-12-13T19:43:50Z	ERROR	controllers.RayCluster	RayCluster update state error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:357
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:50Z	INFO	controllers.RayCluster	updateClusterReason	{"Update CR Status.Reason": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
2023-12-13T19:43:50Z	ERROR	controllers.RayCluster	RayCluster update reason error	{"cluster name": "raycluster-sample", "error": "Put \"https://127.0.0.1:43751/apis/ray.io/v1/namespaces/default/rayclusters/raycluster-sample/status\": dial tcp 127.0.0.1:43751: connect: connection refused"}
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).rayClusterReconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:360
github.com/ray-project/kuberay/ray-operator/controllers/ray.(*RayClusterReconciler).Reconcile
	/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller.go:160
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Reconcile
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:119
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:316
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:50Z	INFO	Warning: Reconciler returned both a non-zero result and a non-nil error. The result will always be ignored if the error is non-nil and the non-nil error causes reqeueuing with exponential backoff. For more details, see: https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/reconcile#Reconciler	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "a1693f42-cc70-4b05-b1f2-be985609f3d1"}
2023-12-13T19:43:50Z	ERROR	Reconciler error	{"controller": "raycluster-controller", "controllerGroup": "ray.io", "controllerKind": "RayCluster", "RayCluster": {"name":"raycluster-sample","namespace":"default"}, "namespace": "default", "name": "raycluster-sample", "reconcileID": "a1693f42-cc70-4b05-b1f2-be985609f3d1", "error": "Get \"https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group\": dial tcp 127.0.0.1:43751: connect: connection refused"}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:329
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:266
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func2.2
	/home/ubuntu/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.16.3/pkg/internal/controller/controller.go:227
2023-12-13T19:43:50Z	DEBUG	events	Get "https://127.0.0.1:43751/api/v1/namespaces/default/pods?labelSelector=ray.io%2Fcluster%3Draycluster-sample%2Cray.io%2Fgroup%3Dsmall-group": dial tcp 127.0.0.1:43751: connect: connection refused	{"type": "Warning", "object": {"kind":"RayCluster","namespace":"default","name":"raycluster-sample","uid":"88165eec-5900-4c38-a006-570b61e22a0b","apiVersion":"ray.io/v1","resourceVersion":"298"}, "reason": "PodReconciliationError"}

[38;5;9m[1mSummarizing 2 Failures:[0m
  [38;5;9m[FAIL][0m [0mInside the default namespace [38;5;243mWhen creating a raycluster [38;5;9m[1m[It] should run all head and worker pods if un-suspended[0m
  [38;5;243m/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller_test.go:411[0m
  [38;5;9m[FAIL][0m [0mInside the default namespace [38;5;243mWhen creating a raycluster [38;5;9m[1m[It] cluster's .status.state should be updated back to 'ready' after being un-suspended[0m
  [38;5;243m/home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/raycluster_controller_test.go:431[0m

[38;5;9m[1mRan 19 of 19 Specs in 80.288 seconds[0m
[38;5;9m[1mFAIL![0m -- [38;5;10m[1m17 Passed[0m | [38;5;9m[1m2 Failed[0m | [38;5;11m[1m0 Pending[0m | [38;5;14m[1m0 Skipped[0m
--- FAIL: TestAPIs (80.29s)
FAIL
coverage: 21.2% of statements
FAIL	github.com/ray-project/kuberay/ray-operator/controllers/ray	80.380s
=== RUN   TestCreatePodGroup
--- PASS: TestCreatePodGroup (0.00s)
PASS
coverage: 9.1% of statements
ok  	github.com/ray-project/kuberay/ray-operator/controllers/ray/batchscheduler/volcano	0.083s	coverage: 9.1% of statements
=== RUN   TestBuildIngressForHeadServiceWithoutIngressClass
time="2023-12-13T19:42:32Z" level=warning msg="ingress class annotation is not set for cluster default/raycluster-sample"
--- PASS: TestBuildIngressForHeadServiceWithoutIngressClass (0.00s)
=== RUN   TestBuildIngressForHeadService
--- PASS: TestBuildIngressForHeadService (0.00s)
=== RUN   TestGetDecodedRuntimeEnv
--- PASS: TestGetDecodedRuntimeEnv (0.00s)
=== RUN   TestGetRuntimeEnvJsonFromBase64
--- PASS: TestGetRuntimeEnvJsonFromBase64 (0.00s)
=== RUN   TestGetRuntimeEnvJsonFromYAML
--- PASS: TestGetRuntimeEnvJsonFromYAML (0.00s)
=== RUN   TestGetRuntimeEnvJsonErrorWithBothFields
--- PASS: TestGetRuntimeEnvJsonErrorWithBothFields (0.00s)
=== RUN   TestGetBaseRayJobCommand
--- PASS: TestGetBaseRayJobCommand (0.00s)
=== RUN   TestGetMetadataJson
--- PASS: TestGetMetadataJson (0.00s)
=== RUN   TestGetK8sJobCommand
--- PASS: TestGetK8sJobCommand (0.00s)
=== RUN   TestGetK8sJobCommandWithYAML
--- PASS: TestGetK8sJobCommandWithYAML (0.00s)
=== RUN   TestMetadataRaisesErrorBeforeRay26
--- PASS: TestMetadataRaisesErrorBeforeRay26 (0.00s)
=== RUN   TestGetDefaultSubmitterTemplate
--- PASS: TestGetDefaultSubmitterTemplate (0.00s)
=== RUN   TestAddEmptyDirVolumes
--- PASS: TestAddEmptyDirVolumes (0.00s)
=== RUN   TestGetHeadPort
--- PASS: TestGetHeadPort (0.00s)
=== RUN   TestBuildPod
--- PASS: TestBuildPod (0.00s)
=== RUN   TestBuildPod_WithOverwriteCommand
--- PASS: TestBuildPod_WithOverwriteCommand (0.00s)
=== RUN   TestBuildPod_WithAutoscalerEnabled
--- PASS: TestBuildPod_WithAutoscalerEnabled (0.00s)
=== RUN   TestBuildPod_WithCreatedByRayService
--- PASS: TestBuildPod_WithCreatedByRayService (0.00s)
=== RUN   TestBuildPod_WithGcsFtEnabled
--- PASS: TestBuildPod_WithGcsFtEnabled (0.00s)
=== RUN   TestBuildPodWithAutoscalerOptions
--- PASS: TestBuildPodWithAutoscalerOptions (0.00s)
=== RUN   TestHeadPodTemplate_WithAutoscalingEnabled
label value is too long: len = 205, we will shorten it by offset = 142
pod name is too long: len = 200, we will shorten it by offset = 150
pod name is too long: len = 200, we will shorten it by offset = 150
--- PASS: TestHeadPodTemplate_WithAutoscalingEnabled (0.00s)
=== RUN   TestHeadPodTemplate_AutoscalerImage
--- PASS: TestHeadPodTemplate_AutoscalerImage (0.00s)
=== RUN   TestHeadPodTemplate_WithNoServiceAccount
--- PASS: TestHeadPodTemplate_WithNoServiceAccount (0.00s)
=== RUN   TestHeadPodTemplate_WithServiceAccountNoAutoscaling
--- PASS: TestHeadPodTemplate_WithServiceAccountNoAutoscaling (0.00s)
=== RUN   TestHeadPodTemplate_WithServiceAccount
--- PASS: TestHeadPodTemplate_WithServiceAccount (0.00s)
=== RUN   TestValidateHeadRayStartParams_OK
--- PASS: TestValidateHeadRayStartParams_OK (0.00s)
=== RUN   TestValidateHeadRayStartParams_ValidWithObjectStoreMemoryError
--- PASS: TestValidateHeadRayStartParams_ValidWithObjectStoreMemoryError (0.00s)
=== RUN   TestValidateHeadRayStartParams_InvalidObjectStoreMemory
--- PASS: TestValidateHeadRayStartParams_InvalidObjectStoreMemory (0.00s)
=== RUN   TestCleanupInvalidVolumeMounts
--- PASS: TestCleanupInvalidVolumeMounts (0.00s)
=== RUN   TestDefaultWorkerPodTemplateWithName
--- PASS: TestDefaultWorkerPodTemplateWithName (0.00s)
=== RUN   TestDefaultHeadPodTemplateWithConfigurablePorts
--- PASS: TestDefaultHeadPodTemplateWithConfigurablePorts (0.00s)
=== RUN   TestDefaultWorkerPodTemplateWithConfigurablePorts
--- PASS: TestDefaultWorkerPodTemplateWithConfigurablePorts (0.00s)
=== RUN   TestDefaultInitContainer
--- PASS: TestDefaultInitContainer (0.00s)
=== RUN   TestDefaultInitContainerImagePullPolicy
=== RUN   TestDefaultInitContainerImagePullPolicy/Always
=== RUN   TestDefaultInitContainerImagePullPolicy/IfNotPresent
=== RUN   TestDefaultInitContainerImagePullPolicy/Never
--- PASS: TestDefaultInitContainerImagePullPolicy (0.00s)
    --- PASS: TestDefaultInitContainerImagePullPolicy/Always (0.00s)
    --- PASS: TestDefaultInitContainerImagePullPolicy/IfNotPresent (0.00s)
    --- PASS: TestDefaultInitContainerImagePullPolicy/Never (0.00s)
=== RUN   TestSetMissingRayStartParamsAddress
--- PASS: TestSetMissingRayStartParamsAddress (0.00s)
=== RUN   TestSetMissingRayStartParamsMetricsExportPort
--- PASS: TestSetMissingRayStartParamsMetricsExportPort (0.00s)
=== RUN   TestSetMissingRayStartParamsBlock
--- PASS: TestSetMissingRayStartParamsBlock (0.00s)
=== RUN   TestSetMissingRayStartParamsDashboardHost
--- PASS: TestSetMissingRayStartParamsDashboardHost (0.00s)
=== RUN   TestSetMissingRayStartParamsAgentListenPort
--- PASS: TestSetMissingRayStartParamsAgentListenPort (0.00s)
=== RUN   TestGetCustomWorkerInitImage
--- PASS: TestGetCustomWorkerInitImage (0.00s)
=== RUN   TestGetEnableProbesInjection
--- PASS: TestGetEnableProbesInjection (0.00s)
=== RUN   TestInitHealthProbe
--- PASS: TestInitHealthProbe (0.00s)
=== RUN   TestBuildRoleBindingSubjectAndRoleRefName
pod name is too long: len = 200, we will shorten it by offset = 150
pod name is too long: len = 200, we will shorten it by offset = 150
=== RUN   TestBuildRoleBindingSubjectAndRoleRefName/Ray_cluster_without_head_group_service_account
=== RUN   TestBuildRoleBindingSubjectAndRoleRefName/Ray_cluster_with_a_long_name_and_without_head_group_service_account
pod name is too long: len = 200, we will shorten it by offset = 150
pod name is too long: len = 200, we will shorten it by offset = 150
=== RUN   TestBuildRoleBindingSubjectAndRoleRefName/Ray_cluster_with_head_group_service_account
--- PASS: TestBuildRoleBindingSubjectAndRoleRefName (0.00s)
    --- PASS: TestBuildRoleBindingSubjectAndRoleRefName/Ray_cluster_without_head_group_service_account (0.00s)
    --- PASS: TestBuildRoleBindingSubjectAndRoleRefName/Ray_cluster_with_a_long_name_and_without_head_group_service_account (0.00s)
    --- PASS: TestBuildRoleBindingSubjectAndRoleRefName/Ray_cluster_with_head_group_service_account (0.00s)
=== RUN   TestBuildRouteForHeadService
--- PASS: TestBuildRouteForHeadService (0.00s)
=== RUN   TestBuildServiceForHeadPod
--- PASS: TestBuildServiceForHeadPod (0.00s)
=== RUN   TestBuildServiceForHeadPodWithAppNameLabel
--- PASS: TestBuildServiceForHeadPodWithAppNameLabel (0.00s)
=== RUN   TestBuildServiceForHeadPodWithAnnotations
--- PASS: TestBuildServiceForHeadPodWithAnnotations (0.00s)
=== RUN   TestGetPortsFromCluster
--- PASS: TestGetPortsFromCluster (0.00s)
=== RUN   TestGetServicePortsWithMetricsPort
--- PASS: TestGetServicePortsWithMetricsPort (0.00s)
=== RUN   TestUserSpecifiedHeadService
default label: key=ray.io/cluster, value=raycluster-sample
default label: key=ray.io/identifier, value=raycluster-sample-head
default label: key=ray.io/node-type, value=head
default label: key=app.kubernetes.io/name, value=kuberay
default label: key=app.kubernetes.io/created-by, value=kuberay-operator
    service_test.go:375: head service: {
          "metadata": {
            "name": "user-custom-name",
            "namespace": "default",
            "creationTimestamp": null,
            "labels": {
              "app.kubernetes.io/created-by": "kuberay-operator",
              "app.kubernetes.io/name": "kuberay",
              "ray.io/cluster": "userTemplateClusterName",
              "ray.io/identifier": "raycluster-sample-head",
              "ray.io/node-type": "head",
              "userLabelKey": "userLabelValue"
            },
            "annotations": {
              "HeadServiceAnnotationKey1": "HeadServiceAnnotationValue1",
              "HeadServiceAnnotationKey2": "HeadServiceAnnotationValue2",
              "userAnnotationKey": "userAnnotationValue"
            }
          },
          "spec": {
            "ports": [
              {
                "name": "userPort",
                "port": 12345,
                "targetPort": 0
              },
              {
                "name": "client",
                "port": 98765,
                "targetPort": 0
              },
              {
                "name": "gcs",
                "appProtocol": "tcp",
                "port": 6379,
                "targetPort": 0
              },
              {
                "name": "8265-port",
                "appProtocol": "tcp",
                "port": 8265,
                "targetPort": 0
              },
              {
                "name": "serve",
                "appProtocol": "tcp",
                "port": 8000,
                "targetPort": 0
              },
              {
                "name": "metrics",
                "appProtocol": "tcp",
                "port": 8080,
                "targetPort": 0
              }
            ],
            "selector": {
              "app.kubernetes.io/created-by": "kuberay-operator",
              "app.kubernetes.io/name": "kuberay",
              "ray.io/cluster": "userTemplateClusterName",
              "ray.io/identifier": "raycluster-sample-head",
              "ray.io/node-type": "head"
            },
            "type": "LoadBalancer"
          },
          "status": {
            "loadBalancer": {}
          }
        }
--- PASS: TestUserSpecifiedHeadService (0.00s)
=== RUN   TestNilMapDoesntErrorInUserSpecifiedHeadService
--- PASS: TestNilMapDoesntErrorInUserSpecifiedHeadService (0.00s)
=== RUN   TestBuildServiceForHeadPodPortsOrder
--- PASS: TestBuildServiceForHeadPodPortsOrder (0.00s)
=== RUN   TestBuildServeServiceForRayService
--- PASS: TestBuildServeServiceForRayService (0.00s)
=== RUN   TestBuildServeServiceForRayCluster
--- PASS: TestBuildServeServiceForRayCluster (0.00s)
=== RUN   TestBuildServeServiceForRayService_WithoutServePort
--- PASS: TestBuildServeServiceForRayService_WithoutServePort (0.00s)
=== RUN   TestUserSpecifiedServeService
--- PASS: TestUserSpecifiedServeService (0.00s)
PASS
coverage: 85.7% of statements
ok  	github.com/ray-project/kuberay/ray-operator/controllers/ray/common	0.090s	coverage: 85.7% of statements
=== RUN   TestGetClusterDomainName
=== RUN   TestGetClusterDomainName/all_good_from_env
=== RUN   TestGetClusterDomainName/No_env_set
--- PASS: TestGetClusterDomainName (0.00s)
    --- PASS: TestGetClusterDomainName/all_good_from_env (0.00s)
    --- PASS: TestGetClusterDomainName/No_env_set (0.00s)
=== RUN   TestBefore
--- PASS: TestBefore (0.00s)
=== RUN   TestStatus
--- PASS: TestStatus (0.00s)
=== RUN   TestCheckAllPodsRunning
time="2023-12-13T19:42:32Z" level=info msg="CheckAllPodsRunning: Pod is not running; Pod Name: raycluster-sample-small-group-worker-0; Pod Status.Phase: Pending"
--- PASS: TestCheckAllPodsRunning (0.00s)
=== RUN   TestCheckName
pod name is too long: len = 69, we will shorten it by offset = 19
-ca41-e903-fc3ae634b18e-lazer090scholar-director-s
pod name is too long: len = 59, we will shorten it by offset = 9
--- PASS: TestCheckName (0.00s)
=== RUN   TestGetHeadGroupServiceAccountName
=== RUN   TestGetHeadGroupServiceAccountName/Ray_cluster_without_head_group_service_account
=== RUN   TestGetHeadGroupServiceAccountName/Ray_cluster_with_head_group_service_account
--- PASS: TestGetHeadGroupServiceAccountName (0.00s)
    --- PASS: TestGetHeadGroupServiceAccountName/Ray_cluster_without_head_group_service_account (0.00s)
    --- PASS: TestGetHeadGroupServiceAccountName/Ray_cluster_with_head_group_service_account (0.00s)
=== RUN   TestReconcile_CheckNeedRemoveOldPod
--- PASS: TestReconcile_CheckNeedRemoveOldPod (0.00s)
=== RUN   TestCalculateAvailableReplicas
--- PASS: TestCalculateAvailableReplicas (0.00s)
=== RUN   TestFindContainerPort
--- PASS: TestFindContainerPort (0.00s)
=== RUN   TestGenerateHeadServiceName
--- PASS: TestGenerateHeadServiceName (0.00s)
=== RUN   TestGetWorkerGroupDesiredReplicas
time="2023-12-13T19:42:32Z" level=warning msg="replicas (6) is greater than maxReplicas (5), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
time="2023-12-13T19:42:32Z" level=warning msg="minReplicas (5) is greater than maxReplicas (1), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
--- PASS: TestGetWorkerGroupDesiredReplicas (0.00s)
=== RUN   TestCalculateDesiredReplicas
=== RUN   TestCalculateDesiredReplicas/Group1's_Replicas_is_more_than_MaxReplicas.
time="2023-12-13T19:42:32Z" level=warning msg="replicas (6) is greater than maxReplicas (5), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
=== RUN   TestCalculateDesiredReplicas/Both_groups'_Replicas_are_nil
=== RUN   TestCalculateDesiredReplicas/Group1's_Replicas_is_smaller_than_MinReplicas,_and_Group2's_Replicas_is_more_than_MaxReplicas.
time="2023-12-13T19:42:32Z" level=warning msg="replicas (6) is greater than maxReplicas (5), using maxReplicas as desired replicas. Please fix this to avoid any unexpected behaviors."
--- PASS: TestCalculateDesiredReplicas (0.00s)
    --- PASS: TestCalculateDesiredReplicas/Group1's_Replicas_is_more_than_MaxReplicas. (0.00s)
    --- PASS: TestCalculateDesiredReplicas/Both_groups'_Replicas_are_nil (0.00s)
    --- PASS: TestCalculateDesiredReplicas/Group1's_Replicas_is_smaller_than_MinReplicas,_and_Group2's_Replicas_is_more_than_MaxReplicas. (0.00s)
=== RUN   TestUtils
Running Suite: Utils Suite - /home/ubuntu/go/src/kuberay/ray-operator/controllers/ray/utils
===========================================================================================
Random Seed: [1m1702496552[0m

Will run [1m4[0m of [1m4[0m specs
[38;5;10mâ€¢[0m[38;5;10mâ€¢[0m[38;5;10mâ€¢[0m[38;5;10mâ€¢[0m

[38;5;10m[1mRan 4 of 4 Specs in 0.004 seconds[0m
[38;5;10m[1mSUCCESS![0m -- [38;5;10m[1m4 Passed[0m | [38;5;9m[1m0 Failed[0m | [38;5;11m[1m0 Pending[0m | [38;5;14m[1m0 Skipped[0m
--- PASS: TestUtils (0.00s)
PASS
coverage: 29.4% of statements
ok  	github.com/ray-project/kuberay/ray-operator/controllers/ray/utils	0.061s	coverage: 29.4% of statements
FAIL
