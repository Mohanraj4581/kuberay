# Default values for ray-cluster.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

image:
  repository: rayproject/ray
  tag: latest
  pullPolicy: IfNotPresent

nameOverride: "ray"
fullnameOverride: ""

imagePullSecrets: []
  # - name: an-existing-secret

head:
  groupName: headgroup
  replicas: 1
  type: head
  labels:
    key: value
  initArgs:
    port: '6379'
    redis-password: 'LetMeInRay' # Deprecated since Ray 1.11 due to GCS bootstrapping enabled
    dashboard-host: '0.0.0.0'
    num-cpus: '1' # can be auto-completed from the limits
    node-ip-address: $MY_POD_IP # auto-completed as the head pod IP
    block: 'true'
  containerEnv:
    - name: MY_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
  envFrom: []
    # - secretRef:
    #     name: my-env-secret
  resources:
    limits:
      cpu: 1
    requests:
       cpu: 200m
  annotations: {}
  nodeSelector: {}
  tolerations: []
  affinity: {}
  volumes:
    - name: log-volume
      emptyDir: { }
  volumeMounts:
    - mountPath: /tmp/ray
      name: log-volume


workers:
  # 1st worker group definition.
  # NOTES:
  # The underlying infrastructure that each worker type definition is run on can be specified using the CPU and MEMORY requests and limits.
  # To explicitely specify underlying compute node types, us the nodeSelector field with dictionary mapping {"agentpool" : #POOLNAME#}, where #POOLNAME# is the 
  # name of the underlying Kubernetes cluster compute name. In cloud based solutions, the node pool is the level at which computer instance types are specified e.g. 
  # number of cpu cores, amount of memory, whether there are gpus etc. 
  worker_type_default:
    groupName: workergroupdefault
    replicas: 1
    minReplicas: 0
    maxReplicas: 4
    type: worker
    labels:
      key: value
    initArgs:
      node-ip-address: $MY_POD_IP
      redis-password: LetMeInRay
      block: 'true'
    containerEnv:
      - name: MY_POD_IP
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      - name: RAY_DISABLE_DOCKER_CPU_WARNING
        value: "1"
      - name: CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            containerName: ray-worker
            resource: requests.cpu
      - name: CPU_LIMITS
        valueFrom:
          resourceFieldRef:
            containerName: ray-worker
            resource: limits.cpu
      - name: MEMORY_REQUESTS
        valueFrom:
          resourceFieldRef:
            containerName: ray-worker
            resource: requests.memory
      - name: MEMORY_LIMITS
        valueFrom:
          resourceFieldRef:
            containerName: ray-worker
            resource: limits.memory
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
    envFrom: []
      # - secretRef:
      #     name: my-env-secret
    ports:
      - containerPort: 80
        protocol: TCP
    # Resources section helps define what resources are available to the Ray cluster.
    resources: 
      limits:
        cpu: 1
      requests:
        cpu: 200m
    annotations:
      key: value
    # Use nodeSelector to specify specific node pools for deployment of these type of worker pods.
    # e.g. {"agentpool" : "raypool"}
    nodeSelector: {}
    tolerations: []
    affinity: {}
    volumes:
      - name: log-volume
        emptyDir: {}
    volumeMounts:
      - mountPath: /tmp/ray
        name: log-volume
  
  # Repeat block above for additional worker types.
  # e.g. 2nd definition of worker types
  #worker_type_large:
  #  groupName: workergroupdefault
  #  replicas: 1
  #  minReplicas: 0
  #  maxReplicas: 4
  #  type: worker
  # ...

headServiceSuffix: "ray-operator.svc"

service:
  type: ClusterIP
  port: 8080
